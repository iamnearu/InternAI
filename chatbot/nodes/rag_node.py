"""
File: rag_node.py
Th∆∞ m·ª•c: chatbot/nodes/

M·ª•c ƒë√≠ch:
- D√πng m√¥ h√¨nh Gemini (Google) ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi RAG.
- Nh·∫≠n context_chunks + c√¢u h·ªèi ‚Üí sinh prompt ‚Üí tr·∫£ v·ªÅ "answer" d·∫°ng dict.
"""
print("üö® RAG_NODE.PY ƒëang ch·∫°y t·ª´:", __file__)

import google.generativeai as genai

# ‚ö†Ô∏è Thay b·∫±ng API Key Gemini c·ªßa b·∫°n
genai.configure(api_key="AIzaSyC6fS5gEB4WIuiXOMY-RRSDJOus8OnbjyU")

# T·∫°o m√¥ h√¨nh Gemini ƒë√∫ng version h·ªó tr·ª£
model = genai.GenerativeModel("gemini-1.5-flash")  # ho·∫∑c "gemini-1.5-pro"

def build_prompt(context_chunks, question):
    """
    Gh√©p c√°c ƒëo·∫°n context v√† c√¢u h·ªèi th√†nh prompt ƒë·∫ßy ƒë·ªß.
    """
    context = "\n".join(context_chunks)
    return f"""D·ª±a v√†o th√¥ng tin CCCD d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi.

Th√¥ng tin CCCD:
{context}

C√¢u h·ªèi: {question}
Tr·∫£ l·ªùi:"""

def rag_node(state: dict) -> dict:
    """
    Node RAG: Nh·∫≠n context_chunks + question ‚Üí sinh answer b·∫±ng Gemini.

    Tr·∫£ v·ªÅ:
        dict: {"answer": ...}
    """
    context_chunks = state.get("context_chunks", [])
    question = state.get("question", "")

    prompt = build_prompt(context_chunks, question)
    if not context_chunks:
        return {"answer": "‚ùå Kh√¥ng c√≥ th√¥ng tin context t·ª´ ·∫£nh CCCD. OCR/Caption ƒë·ªÅu th·∫•t b·∫°i."}

    try:
        response = model.generate_content(prompt)
        return {"answer": response.text}
    except Exception as e:
        return {"answer": f"L·ªói khi g·ªçi Gemini API: {str(e)}"}  # ‚úÖ B·ªçc l·ªói trong dict
